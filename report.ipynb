{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0687806e152625f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde5b5353277580",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here is the code snippet for the dqn algorithm.\n",
    "```python\n",
    "def dqn(agent, env, n_episodes=100, max_t=500):\n",
    "\n",
    "    scores_window = deque(maxlen=100)\n",
    "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
    "\n",
    "    scores_over_episodes=[]\n",
    "\n",
    "    ''' initialize epsilon '''\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores_window.append(score)\n",
    "        scores_over_episodes.append(score)\n",
    "\n",
    "        ''' decrease epsilon '''\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tNumber of steps: {}'.format(i_episode, np.mean(scores_window), t), end=\"\")\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \"\"\"if np.mean(scores_window)>=195.0:\n",
    "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "           break\"\"\"\n",
    "\n",
    "\n",
    "    return scores_over_episodes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae31fdfafe658dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T05:16:30.663231Z",
     "start_time": "2024-04-06T05:16:27.495240800Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from duelingDQN import dqn, DuelingDQNAgent\n",
    "import matplotlib.pyplot as plt\n",
    "from vars import *\n",
    "import numpy as np\n",
    "\n",
    "ENVS = [HyperEnv('CartPole-v1', 150, 1000, 128, 1e-4, 100, 0.99, 0.1, 128, 64), HyperEnv('Acrobot-v1', 200, 500, 256, 1e-3, 500, 0.99, 0.1, 128, 256)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56484c37a67506ba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## DDQN Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546274e2b7d746b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Dueling DQN algorithm with mean aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7ef6aecf82217",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-06T05:16:30.663328900Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating Dueling DQN agent on CartPole-v1 environment:\n",
      "Episode 100\tScore: 178.00\n",
      "Episode 100\tScore: 153.00\n",
      "Episode 131\tScore: 216.00"
     ]
    }
   ],
   "source": [
    "cartpole_parameters = ENVS[0]\n",
    "\n",
    "print(f\"\\nTraining and evaluating Dueling DQN agent on {cartpole_parameters.name} environment:\")\n",
    "env = gym.make(cartpole_parameters.name)\n",
    "\n",
    "rewards = []\n",
    "for _ in range(5):\n",
    "    dueling_dqn_agent = DuelingDQNAgent(env.observation_space.shape[0], env.action_space.n, 37, \"mean\", cartpole_parameters)\n",
    "    scores = dqn(dueling_dqn_agent, env, n_episodes=cartpole_parameters.n_episodes, max_t=cartpole_parameters.max_t)\n",
    "    rewards.append(scores)\n",
    "avg_reward = np.array(rewards).mean(0)\n",
    "std_reward = np.array(rewards).std(0)\n",
    "\n",
    "x_axis = np.arange(len(rewards[0]))\n",
    "plt.fill_between(x_axis, avg_reward - std_reward, avg_reward + std_reward, alpha=0.5)\n",
    "plt.title(\"Reward vs Episode\")\n",
    "plt.plot(avg_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8751feefe78b6c2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Dueling DQN algorithm with max aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d71ba80625ee8e",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cartpole_parameters = ENVS[0]\n",
    "seed = 37\n",
    "\n",
    "print(f\"\\nTraining and evaluating Dueling DQN agent on {cartpole_parameters.name} environment:\")\n",
    "env = gym.make(cartpole_parameters.name)\n",
    "\n",
    "rewards = []\n",
    "for _ in range(5):\n",
    "    dueling_dqn_agent = DuelingDQNAgent(env.observation_space.shape[0], env.action_space.n, seed, \"max\", cartpole_parameters)\n",
    "    scores = dqn(dueling_dqn_agent, env, n_episodes=cartpole_parameters.n_episodes, max_t=cartpole_parameters.max_t)\n",
    "    rewards.append(scores)\n",
    "avg_reward = np.array(rewards).mean(0)\n",
    "std_reward = np.array(rewards).std(0)\n",
    "\n",
    "x_axis = np.arange(len(rewards[0]))\n",
    "plt.fill_between(x_axis, avg_reward - std_reward, avg_reward + std_reward, alpha=0.5)\n",
    "plt.plot(avg_reward)\n",
    "plt.title(\"Reward vs Episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be7b100bd6cefa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## DDQN Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137df3e0138791c5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Dueling DQN algorithm with mean aggregation on Acrobot environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195c457c4d13458",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "acrobot_parameters = ENVS[1]\n",
    "\n",
    "print(f\"\\nTraining and evaluating Dueling DQN agent on {ENVS[1].name} environment:\")\n",
    "env = gym.make(acrobot_parameters.name)\n",
    "rewards = []\n",
    "for _ in range(5):\n",
    "    dueling_dqn_agent = DuelingDQNAgent(env.observation_space.shape[0], env.action_space.n, 37, \"mean\", acrobot_parameters)\n",
    "    scores = dqn(dueling_dqn_agent, env, n_episodes=acrobot_parameters.n_episodes, max_t=acrobot_parameters.max_t)\n",
    "    rewards.append(scores)\n",
    "avg_reward = np.array(rewards).mean(0)\n",
    "std_reward = np.array(rewards).std(0)\n",
    "\n",
    "x_axis = np.arange(len(rewards[0]))\n",
    "plt.fill_between(x_axis, avg_reward - std_reward, avg_reward + std_reward, alpha=0.5)\n",
    "plt.plot(avg_reward)\n",
    "plt.title(\"Reward vs Episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc7932eae6cb95",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Dueling DQN algorithm with max aggregation on Acrobot environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f9a2b1d352a5f",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "acrobot_parameters = ENVS[1]\n",
    "\n",
    "print(f\"\\nTraining and evaluating Dueling DQN agent on {ENVS[1].name} environment:\")\n",
    "env = gym.make(acrobot_parameters.name)\n",
    "rewards = []\n",
    "for _ in range(5):\n",
    "    dueling_dqn_agent = DuelingDQNAgent(env.observation_space.shape[0], env.action_space.n, 37, \"max\", acrobot_parameters)\n",
    "    scores = dqn(dueling_dqn_agent, env, n_episodes=acrobot_parameters.n_episodes, max_t=acrobot_parameters.max_t)\n",
    "    rewards.append(scores)\n",
    "avg_reward = np.array(rewards).mean(0)\n",
    "std_reward = np.array(rewards).std(0)\n",
    "\n",
    "x_axis = np.arange(len(rewards[0]))\n",
    "plt.fill_between(x_axis, avg_reward - std_reward, avg_reward + std_reward, alpha=0.5)\n",
    "plt.plot(avg_reward)\n",
    "plt.title(\"Reward vs Episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20a88f9b00de11",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## DDQN Inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a004cfd6f8defd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## REINFORCE cartpole"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
